# TypeDB Knowledge Graph for Question Answering

Using TypeDB as a knowledge graph backend for multi-hop question answering, evaluated on the [2WikiMultihopQA](https://github.com/Alab-NII/2wikimultihop) benchmark.

Two pipelines are provided: **TypeQL** (LLM generates TypeQL directly) and **GraphRAG** (LLM extracts a flat graph, answers via RAG).

## Dataset

We use the [2WikiMultihopQA](https://github.com/Alab-NII/2wikimultihop) benchmark. Place dataset files (e.g. `dev.json`) under `data/2wikimultihopqa/`.

### Extracting questions and sources

`helpers/2wikimultihopqa/extract.py` extracts questions or source page titles from the dataset:

```bash
# Extract questions (one per line)
python helpers/2wikimultihopqa/extract.py data/2wikimultihopqa/dev.json questions -o questions.txt

# Extract source page titles (one JSON list per line)
python helpers/2wikimultihopqa/extract.py data/2wikimultihopqa/dev.json sources -o sources.jsonl

# Sample 10 random examples
python helpers/2wikimultihopqa/extract.py data/2wikimultihopqa/dev.json questions -n 10 --seed 42
```

## LLM backends

All LLM-calling scripts support two backends:

- **Claude CLI** (`--claude`): Pipes prompts to `claude -p` via stdin. Requires Claude Code CLI to be installed.
- **Local llama-cpp** (default): Calls a local OpenAI-compatible completion endpoint (`--url`, default `http://localhost:8080/v1`).

## Shared tools

### Fetch schema

Fetch and print the schema from a TypeDB database:

```bash
python -m typedb_kgqa.fetch_schema -d my_database
python -m typedb_kgqa.fetch_schema -d my_database --compact
```

### Ingest documents

Load paragraphs from a dataset JSON file into TypeDB as `meta-document` entities (with `meta-page-title` and `text-content`). Deduplicates by title.

```bash
python -m typedb_kgqa.ingest_documents --dataset data/2wikimultihopqa/dev.json -d my_database
```

## TypeQL Pipeline

Uses a richly typed, domain-specific schema (`base-schema.tql` + `2wmhqa.tql`). 
`base-schema.tql` was handwritten to guide the , inspired by the idea of [Basic Formal Ontology](https://basic-formal-ontology.org/).
`2wmhqa.tql` was generated by Claude, extending `base-schema.tql` based on the questions in the dev set, and revised by humans.
The constructed knowledge graph must capture all facts of interests.
The LLM generates a query which directly answers the question, rather than retrieving documents and reasoning on the text.

### Construct knowledge graph

Read a sources file, fetch document text from TypeDB, and use an LLM to generate 
TypeQL `put` statements for any information that can be encoded by the types in the schema:

```bash
python -m typedb_kgqa.typeql_construction --sources sources.jsonl -d my_database --claude -o inserts.tql
python -m typedb_kgqa.typeql_construction --sources sources.jsonl -d my_database --url http://localhost:8080/v1 -o inserts.tql
```

Prompt: `src/typedb_kgqa/prompts/typeql_construction.txt`

### Generate queries

Read a questions file and use an LLM to generate TypeQL `match` queries which directly answer the question:

```bash
python -m typedb_kgqa.typeql_generate_query -q questions.txt -p src/typedb_kgqa/prompts/typeql_generate_query.txt -d my_database --claude -o queries.tql
python -m typedb_kgqa.typeql_generate_query -q questions.txt -p src/typedb_kgqa/prompts/typeql_generate_query.txt -d my_database -o queries.tql
```

## GraphRAG Pipeline
Uses a flat schema with `node-label` attributes (`graphrag-schema.tql`) to construct a vanilla GraphRAG pipeline.
 The LLM extracts entities, properties, and relations in a simplified line format, which is converted to TypeQL via `lines_to_typeql()`.
Questions are answered by an initial vector-lookup for nodes, and then expanding to the most relevant neighbours.

### Construct knowledge graph

Read a sources file, extract entities/relations via LLM, convert to TypeQL, and write to TypeDB:

```bash
python -m typedb_kgqa.graphrag_construction --sources sources.jsonl -d my_database --claude
python -m typedb_kgqa.graphrag_construction --sources sources.jsonl -d my_database --url http://localhost:8080/v1 --dry-run
```

Prompt: `src/typedb_kgqa/prompts/graphrag_construction.txt`

### Answer questions (RAG)

Embed the question, retrieve relevant documents from TypeDB via graph-aware similarity search, and generate an answer:

```bash
python -m typedb_kgqa.graphrag_answer -q questions.txt -d my_database --claude
python -m typedb_kgqa.graphrag_answer -q questions.txt -d my_database --url http://localhost:8080/v1 -o answers.txt
```

Prompt: `src/typedb_kgqa/prompts/graphrag_answer.txt`

The process is roughly:
1. Finding nodes with embeddings most similar to the query.
2. Expanding to the neighbours of this node which have the highest similarities.
3. Fetching the documents which contained the information which created these nodes.
4. Feeding these to documents to an LLM along with the question.
Since vector-indexing is not yet supported in TypeDB, 
we use a [TypeDB branch](https://github.com/krishnangovindraj/typedb/tree/fuzzy-match) 
which implements cosine similarity as a builtin, and a schema function which naively returns nodes in descending order of similarity.
